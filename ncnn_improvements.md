# Strategy for Raspberry Pi Real-Time Object Detection using YOLOv11n NCNN Model

## 1. Code Structure

Key design principles include **model abstraction**, isolated threads for each task, and a clear folder/module structure. Below is a proposed project structure with each module's responsibility:

- **Camera Interface (`camera/camera.py`):** Handles camera initialisation and frame capture (Piecamera2). Provides a `Camera` class or similar that abstracts the IMX219 camera, encapsulating resolution settings and the `read()` method for grabbing frames.
- **Detectors:** Contains a **base detector interface** (e.g. `base.py`) and specific implementations for each backend:
    - `yolo_ncnn.py` – loads the YOLOv11n NCNN model, handles inference. Both implement a common interface (e.g. `Detector.detect(frame)` returning detections) so they can be swapped easily.
- **Processing:** Contains logic for analyzing frames and selecting the best one:
    - `image_quality.py` – utility functions for blur detection, motion estimation, brightness/entropy calculation, etc.
    - `frame_selector.py` – logic to score frames and decide which frame to save (see Section 4). This decouples "best frame" decision-making from the main loop.
- **Thread Handlers**: Classes or functions to run each pipeline component in its own thread:
    - `capture_thread.py` – continuously grabs frames from the camera and puts them into a thread-safe queue.
    - `inference_thread.py` – pulls frames from the queue and runs the detector's `detect()` method.
    - `save_thread.py` – receives frames flagged as "to save" and writes them to disk (to avoid disk I/O blocking the main loop).
- **Main Application (`main.py`):** Orchestrates initialisation and starts the threads. It would create the camera and model objects, start threads, and handle graceful shutdown (joining threads, releasing camera). This keeps the high-level flow in one place.

For example, the folder structure might look like:

project_root/
│   └── camera.py            # Camera capture interface (Picamera2 wrapper)
│   ├── base.py              # Abstract Detector class (common API for detect)
│   └── yolo_ncnn.py         # NCNN YOLOv11n implementation
│   ├── image_quality.py     # Blur, motion, brightness analysis functions
│   └── frame_selector.py    # Frame scoring and selection logic
│   ├── capture_thread.py    # Thread target for continuous frame capture
│   ├── inference_thread.py  # Thread target for running model inference
│   └── save_thread.py       # Thread target for saving images to disk
│   ├── performance_metrics.py # Tracks system resources and operational metrics
│   └── main.py              # Main script to tie everything together
│   └── config_manager.py    # Manages configuration parameters
│   └── assistance.py        # Helper functions and utilities
│   └── installations.py     # Checks whether all the installations are done
│   └── verifications.py     # Checks whether all the things (camera and the code) are working
│   └── convert_model.py     # Converts PyTorch models to NCNN format
│   └── update_model.py      # Updates model fallback mechanism
│   └── models/              # Models will be saved here
│   └── images/              # Captured Images will be saved here

Each module has a single responsibility, making it easier to modify or replace one component without affecting others. 

**Threaded Pipeline Design:** To achieve real-time performance, decouple the camera, inference, and saving into separate threads or asynchronous tasks. This **producer–consumer pattern** ensures the camera can capture at full frame rate independently of the inference speed. A possible design:

- **Capture Thread (Producer):** Continuously captures frames from the camera (using `Picamera2.capture_array()` or a video stream) and places them into a **frame queue** (e.g. a `queue.Queue`). This thread should run at the camera's maximum rate (or a specified FPS), independent of detection. The OpenCV capture call is a blocking I/O operation that can slow down a single-threaded loop, so putting it in its own thread allows overlap of capture and processing.
- **Inference Thread (Consumer):** Pulls frames from the frame queue and runs object detection on each frame using the selected model backend. Because the heavy computation (PyTorch or NCNN inference) releases the GIL (it happens in C/C++ code), Python threads can run these in parallel without stalling each other. This thread would use the abstract detector interface, so it doesn't need to know if the model is PyTorch or NCNN – that detail is hidden in the `Detector` implementation.
- **Saving Thread:** When the inference thread determines a frame is "capture-worthy" (based on detection of the target object and frame quality criteria), it pushes that frame (or its index) to a **save queue**. The saving thread consumes from this queue and handles writing images to disk (using OpenCV `imwrite` or similar). Disk I/O can be slow, so isolating it ensures the inference loop isn't blocked waiting for file writes. This is especially important on a Pi (SD card writes are slow – an SSD is recommended for sustained logging).
- **(Optional) Display/GUI Thread:** If a live preview with annotations is needed, a separate thread can handle rendering frames to a display or web stream (using the output of the inference thread). This prevents slow GUI refresh from affecting capture or detection.

Using Python's `threading` library with queues (or `concurrent.futures.ThreadPoolExecutor` for simplicity) is effective. The GIL won't hinder throughput in this case because camera I/O and NumPy/OpenCV operations happen outside the GIL or release it. Empirically, moving frame capture and display to their own threads can greatly increase the main processing loop's iteration rate. This design aligns with known patterns that overlap I/O and computation for maximum throughput.

In summary, this redesigned code structure provides a clean separation between camera capture, detection logic, and image saving. It improves maintainability (each part can be developed/tested independently) and allows easier optimization (e.g., swapping in NCNN backend, tuning thread priorities, etc.) while maximising parallelism in a resource-constrained device.

## 2. Frame Quality Strategy

Capturing the **most stable, non-blurry frame** of a moving object is crucial for image quality. This requires real-time analysis of frame quality and smart selection of when to capture. We propose a strategy combining **motion-blur detection algorithms** with a buffering system:

- **Blur Detection:** Use the **variance of the Laplacian** method to quantify image sharpness. This involves computing the Laplacian of the grayscale frame and measuring its variance. A high variance indicates lots of high-frequency detail (sharp image), while a low variance indicates a blurry image. This is a lightweight computation with OpenCV. By setting a threshold on this blur score, the system can reject frames that are definitely blurred. (The current code already uses this method with `cv2.Laplacian(...).var()` as a blur metric, which is a sound approach.)
- **Motion Analysis (Stability):** Detect rapid movement between frames to avoid capturing during motion. A simple technique is **frame differencing**: compute the absolute difference between the current frame and the previous frame and take the mean intensity of the difference. If the difference is large, the scene has changed significantly (object in motion) – such frames are more likely to suffer motion blur or be in the middle of an action (hand moving fast). A low difference means the scene is relatively stable. We can score each frame's "stability" as the inverse of this difference (lower difference = higher stability score). More advanced methods could use optical flow to estimate object motion vector magnitude, but that's heavier to compute on a Pi. The frame-to-frame difference is a good heuristic for detecting motion spikes with minimal overhead.
- **Frame Buffering and Scoring:** Instead of immediately deciding to save or discard a frame, maintain a short **queue of recent frames** along with their quality metrics (blur, stability, etc.). For example, keep the last N frames (perhaps N=5) in a buffer. As each new frame comes in, compute its blur score, motion difference from previous frame, brightness, etc., and store it. The system can then decide in real-time which frame from this buffer is best to capture. This helps when the object is moving: one of a sequence of frames might be notably sharper than the others. By comparing a few consecutive frames, we increase the chance of picking a clear one, rather than relying on a single frame timing. This technique is akin to a "burst capture" in smartphone cameras, where multiple frames are taken and the sharpest one is chosen. The buffer need not be large – just enough to cover the duration of typical motion blur or camera shutter period (a few hundred milliseconds).
- **Trigger Logic:** When the object of interest (e.g. a hand or person) is detected, we don't necessarily capture the very same frame that produced the detection. Instead, detection can trigger a **burst capture mode** for the next few frames, which are all analysed. Among those, pick the best one to save (see Section 4 for selection criteria). This way, if the detection was on a slightly blurry frame, a subsequent frame a few tens of milliseconds later (when the hand perhaps paused or the camera autofocus/exposure settled) might be clearer and chosen for saving. The current system imposes a 3s delay between captures to avoid duplicates – we can keep a delay, but within the allowed capture, we leverage burst + select to maximize quality.
- **Vision-Based Heuristics:** Incorporate additional checks to ensure clarity:
    - **No Capture During Fast Motion:** If the stability score indicates large movement (e.g., hand waving quickly), the system can defer capturing for a moment until motion slows. For instance, require the frame difference to drop below a threshold for at least one frame before allowing a capture. This ensures we don't snap a photo at the worst time (mid-swing).
    - **Brightness and Exposure Check:** Extremely dark or bright frames can be low quality. Compute the average brightness and perhaps the entropy of the brightness. Entropy here measures how evenly distributed pixel intensities are – a well-exposed image tends to have a broad histogram (higher entropy) whereas an over/underexposed image has most pixels clumped at extremes (low entropy). We want reasonably well-lit frames. If the camera's auto-exposure is adjusting, maybe skip the first frame after a sudden change (which could be over or under-exposed) and use the next frame once exposure stabilizes. The current code uses a combined brightness entropy score as a lighting metricfile-avuafclm3umfxcwbkv69mx; we can continue using that to prefer frames with balanced exposure.
    - **Edge Clarity:** We can also run a quick edge detector (like Canny) on the central region of the frame and count strong edges. A sharp frame will produce more and clearer edges than a blurry frame. This is similar to the Laplacian method, just another way to double-check sharpness[github.com](https://github.com/cansik/sharp-frame-extractor#:~:text=The%20idea%20of%20the%20extractor,the%20focus%20of%20the%20scene). This could be part of the scoring system for frames.
    - **Lightweight AI Filter (optional):** If needed, one could train a tiny classifier to distinguish blurry vs sharp images or detect motion blur, but this might be overkill on Raspberry Pi. Simpler statistical measures as described are usually sufficient. There are also algorithms to estimate blur kernel or detect the direction of motion blur, but those are computationally expensive. We favour simple heuristics that can run in a few milliseconds.
- **Camera Settings Tuning:** Although not purely algorithmic, it's worth noting: using a faster shutter speed on the camera can reduce motion blur. The IMX219 camera's exposure time can be adjusted via Piecamera2 controls. In a well-lit kitchen environment, setting a lower exposure (to capture sharper frames of moving hands) is beneficial, at the cost of possibly higher ISO noise. This trade-off can be managed by adding lighting or accepting a bit of noise (which doesn't hinder detection much). The key is that a short exposure freezes motion (reducing blur), making it easier to get a "stable" frame. So the strategy is: if motion is detected, maybe temporarily increase shutter speed (if API allows) or simply always use a relatively frame rate mode to inherently have shorter exposures. This hardware tweak complements the software strategy of selecting the least blurry frame.
- **Frame Queues:** Use queues to pipeline frames through processing stages. For example, a queue from capture thread to an intermediate "quality assessor" thread that calculates blur/motion metrics, then another queue to the inference thread. However, to keep things lightweight, it might be enough to do the quality assessment in the inference thread (since we have the frame there anyway). The inference thread can decide "is this frame good enough to consider for saving?" by checking blur and stability scores before running detection, or immediately after detection. The current system does quality checks after detection and then saves or not based on. We will enhance that by not just thresholding one frame, but scoring multiple frames and picking the best.

In summary, the system will **score each frame in real-time on multiple quality criteria** and use a small buffer to ensure that when a detection (e.g., a hand) occurs, we can choose the clearest recent frame to save. By avoiding captures at moments of high motion and preferring high-sharpness frames, we'll significantly increase the odds that saved images are crisp and clear, even when objects are moving in front of the camera.

## 3. Stability with High FPS

Increasing the processing frame rate from ~2 FPS (PyTorch) to ~10 FPS (NCNN) is great for capturing more detail, but it introduces new considerations to maintain system stability and detection accuracy. Key strategies include frame management policies and load balancing:

- **Maintain a Stable Pipeline (Avoid Backlogs):** With the camera possibly delivering frames faster than the detector can handle, it's crucial to prevent an ever-growing backlog of frames. The solution is to use a **bounded frame queue** (e.g., max length 1 or 2). If the capture thread produces frames at 30 FPS but inference is 10 FPS, the queue will fill up. A bounded queue will block the capture thread or overwrite/drop frames when full. A common approach is to use a queue of size 1 and always push the latest frame, effectively **skipping frames** when the detector is busy. This way, the detector always processes the most recent frame available and old frames are discarded if they couldn't be processed in time. The result is a stable latency (bounded by one frame time) and no memory buildup. It does mean not every frame is analysed, but at 10 FPS processing, that's fine. The priority is real-time responsiveness over sheer frame coverage.
- **Frame Skipping Policy:** The system can implement a policy to dynamically drop or skip frames based on load. For example, measure the inference time; if it starts to approach the frame interval, signal the camera thread to slow down capture or drop intermediate frames. In practice, using the bounded queue approach above achieves this automatically (frames get overwritten if not processed). Another tactic is **variable frame rate**: if nothing interesting is happening, process fewer frames (to save CPU), but when motion or a potential event is detected, temporarily increase processing rate (if possible) to not miss it. This can be complex to tune; a simpler rule is: only process a new frame once the previous frame's detection is done (which is inherently what a single consumer thread does). Since NCNN at ~7–10 FPS is near real-time, the backlog risk is lower, but still, the safe design is to ensure no unlimited queues.
- **Priority Queue for Frames (Quality-Aware Processing):** If we assign quality scores to frames (as discussed in Section 2), we could introduce a mechanism to prioritise sharper frames for processing. For instance, if the capture thread is producing frames faster than we can detect, it might drop frames that are visibly very blurry and only keep better ones. This could be done by having a small buffer where each new frame's blur is checked; if it's extremely blurred and a next frame is incoming, maybe skip processing the blurry one. However, this might be over-optimization – the simpler method of always taking the latest frame works well to reduce latency. Priority can be more relevant in deciding which frame to *save* rather than which to *process*, since detection generally should run on each frame in sequence for continuous tracking. So, using a normal FIFO queue for inference is fine, and use priority logic in the saving stage (where we might have multiple candidate frames to choose from).
- **Thread Coordination and Sync:** Using separate threads requires careful sync but Python's `queue.Queue` makes this straightforward (with internal locking). The main thread (or a monitor thread) can observe the length of queues and processing times. If the system is overwhelmed (CPU 100% and lag increasing), one could reduce the camera capture rate. This can be done by adjusting Piecamera2 frame rate or inserting small sleeps in capture loop when queue is full. Essentially, **feedback control** to maintain stability: e.g., if average processing FPS drops below a target, reduce input rate slightly.
- **Load Balancing Among Threads:** On a 4-core Raspberry Pi, we want to utilize cores without oversubscribing. In our design:
    - The **camera thread** is mostly I/O wait (grabbing frame from camera), which is light on CPU.
    - The **inference thread** will be CPU-heavy (running the NCNN model). NCNN by default can leverage multiple cores via threads internally, but you can also configure the number of threads for NCNN (often it uses all cores by default for convolution operations). We should ensure NCNN is set to use 2–4 threads internally to fully utilize the CPU. This means while one NCNN inference is running, it can parallelize across cores. The rest of the Python threads (camera, saving) will use very little CPU in comparison, so they won't starve the model.
    - The **saving thread** does some CPU work (JPEG encoding via OpenCV and writing to disk). JPEG encoding is also a CPU task that could spike usage briefly. However, since saving is sporadic (only when an event frame is chosen) and the inference thread is continuously running, they will naturally time-share. We should avoid saving too frequently (which is already handled by capture delay). If saving starts taking noticeable time (say writing a large image), we might see a momentary dip in inference throughput, but the overall pipeline remains stable because the capture queue will simply hold the latest frame during that moment.
    
    With this design, the Pi's cores will be fairly balanced: one core often handling camera I/O and miscellaneous, one or two cores busy with NCNN, and occasionally one handling save. The OS scheduler will distribute threads – we typically don't need to manually set affinity, though we could if we find one thread needs a dedicated core.
    
- **Ensuring Accuracy at Higher FPS:** When frames come in faster, one concern is that detection results might fluctuate more (since there are more frames of the moving object, the detector might output slightly different bounding boxes or confidences each frame). To maintain **accuracy and consistency**, consider smoothing or filtering the detection outputs:
    - For example, if the object is the same and appears in multiple frames, you can average the detection confidence or use a simple tracker to ensure the object identity persists. This prevents rapid on/off detection if one frame momentarily had lower confidence. However, YOLO models are generally stable, and at 10 FPS the differences frame-to-frame should be small.
    - Another tactic: if the goal is to capture one image of the object, you don't need to capture all 10 frames per second – one good frame is enough. So the system might intentionally **ignore some frames (frame skipping)** once a high-confidence detection is confirmed, just to reduce redundant processing. For instance, after a detection, you might skip processing for 0.1s while you decide on the best frame to save from that burst. This acts like a cool-down that also balances load.
- **Frame Timing and Thread Communication:** Use timestamps or frame indices to trace how long each frame takes through the pipeline. This can help ensure that increasing FPS doesn't introduce unexpected latency. Ideally, even at 10 FPS, end-to-end latency (capture to result) should be low (100ms or so). If latency grows, it means backlog – which the above strategies (bounded queue, frame dropping) are designed to avoid. We essentially want a system where each frame is processed nearly in real-time and then thrown away (or saved) before the next ones, rather than piling up.
- **Resource Monitoring:** Continue to monitor CPU, RAM, and temperature as was done (psutil). At higher FPS, CPU usage will rise. We should make sure we don't overheat the Pi (consider a heat sink/fan if running continuously at 100% CPU). Stability isn't just about code — it's also about hardware not throttling. The code can log if CPU temp goes high, and one could even intentionally throttle frame rate if temperature crosses a threshold to protect the hardware (a precaution for 24/7 deployments).

By implementing these measures, the system will remain stable at ~10 FPS: no memory leaks or queue buildup, reasonable CPU utilization spread across threads, and consistent detection performance. The **net effect** is that the increased FPS will provide more opportunities to catch the object in a good pose, without overwhelming the system or producing duplicated low-value images. We effectively "tune" the pipeline to handle the higher frame rate by controlling input and processing rates, ensuring accuracy is maintained or even improved (thanks to more frame samples and our intelligent skipping of bad frames).

## 4. Best Frame Selection

When multiple frames are captured in a short burst, we need a robust mechanism to pick the **single best frame** to save (the one with highest quality and most relevant content). Rather than a simple threshold on one frame, we will use a **frame ranking strategy** that evaluates frames on several criteria and selects the top-scoring frame from the buffer. Here's how to do it and a comparison of possible strategies:

- **Quality Metrics for Ranking:** Each frame can be given a score based on:
    - **Sharpness:** As discussed, use Laplacian variance or similar to quantify sharpness (higher is better)[medium.com](https://medium.com/@nasuhcanturker/a-practical-way-to-detect-blurry-images-python-and-opencv-16c0a99f51df#:~:text=autonomous%20vehicles%2C%20drones%2C%20or%20robots,which%20parts%20cannot%20be%20used)[medium.com](https://medium.com/@nasuhcanturker/a-practical-way-to-detect-blurry-images-python-and-opencv-16c0a99f51df#:~:text=,CV_64F). We can normalize this score relative to what's typically "sharp" for our camera (perhaps by empirical calibration).
    - **Motion Blur (Stability):** Frames where the object moved less (between previous and next frame) are better. For instance, if we have a trio of frames around the moment of capture, the middle frame might have the least motion blur if it's when the object paused. Using the mean inter-frame difference, we prefer frames with **lower** difference (meaning more stable)file-avuafclm3umfxcwbkv69mx.
    - **Brightness & Contrast:** A frame with proper exposure (not too dark or bright) and good contrast is preferable. We can score brightness by how close the average is to mid-level and entropy of the histogram[theobjects.com](https://www.theobjects.com/dragonfly/dfhelp/2022-2/Content/Processing%20Images/Image%20Quality%20Metrics.htm#:~:text=Shannon%20Entropy). A higher Shannon entropy of the image usually means more information content (neither all-black nor all-white, and not flat) – which often correlates with better focus and detail.
    - **Framing/Content:** If using detection, consider the properties of the detection:
        - **Detection Confidence:** If our object detector assigns a confidence, that can be a proxy for frame quality – a sharper, clearer view of the object often yields higher confidence. So, frames with the highest detection confidence for the target object could be ranked higher.
        - **Object Size/Completeness:** If the object (e.g., a hand) is only partially in one frame and fully in another, the frame where it's fully visible is better. We can check the bounding box coordinates: e.g., if the box is cut off at the edge in one frame but centered in another, prefer the latter. Similarly, a larger bounding box (to a point) might indicate the object is closer or more fully captured.
        - These content-based criteria ensure we pick a frame that not only is clear but also has the object well-presented.
- **Composite Scoring:** We can combine the above metrics into a single score for each frame. For example, define:
    
    `score = w1*(sharpness_norm) + w2*(stability_norm) + w3*(exposure_norm) + w4*(confidence_norm)`
    
    where each sub-score is normalised to a comparable scale (0 to 1) and weighted by importance. We might give sharpness a high weight, since motion blur is the primary concern, and also confidence a high weight to ensure the object is well-captured. The weights can be tuned based on tests (or even learned if we had a small training set of what "good" vs "bad" captures are). The highest scoring frame in the recent buffer is selected as the "best frame" to save.
    
- **Multi-Frame Buffer & Aging:** The buffer of frames (say last 5 frames) should be updated as time progresses. We use an **aging policy** so that old frames don't linger beyond their usefulness:
    - For instance, once an object detection event has passed (no detection for some frames), we flush the buffer because those frames are no longer relevant.
    - Or simply use a FIFO of fixed size – when a new frame comes in, if the buffer is full, drop the oldest. This ensures we're always focusing on the latest frames. In practice, for each detection event we might start filling a buffer for a short window (e.g., 0.5 second) and then freeze selection.
    - An "event" could be defined as the period from when an object is first detected until it's no longer detected. During that period, continuously evaluate frames. If we want only one image per event, we might choose the best frame at the end of the event (or when a good one appears) and then ignore the rest until a new event starts. This relates to the capture delay logic (to avoid saving multiple similar images of the same person/hand).
    - If continuous capture is desired, we could instead pick the best frame every X seconds regardless of events, but the question suggests focusing on when an object is present.
- **Comparison of Frame Ranking Strategies:** The table below summarises different strategies and their pros/cons:

| **Frame Ranking Strategy** | **Methodology** | **Pros** | **Cons/Considerations** |
| --- | --- | --- | --- |
| **Max Sharpness (Focus)** | Select the frame with the highest Laplacian variance (sharpness measure). | Ensures the chosen frame is as crisp as possible in terms of focus and motion blur. Easy to compute. | May pick a frame where the object isn't in the best position (e.g., if it happened to focus on background). Doesn't account for content (could select a sharp frame with no object if detector false positive triggered). |
| **Minimum Motion (Stability)** | Select the frame with the lowest change compared to adjacent frames (using mean frame difference or optical flow). | Likely to avoid frames captured mid-swing or rapid motion, reducing motion blur. Good for picking moments of stillness. | A perfectly still frame might occur when the object is absent or just before it enters – must combine with detection to ensure object is actually present. On its own, could favour an irrelevant frame. |
| **Best Exposure (Brightness)** | Select frame with balanced brightness/contrast – e.g., highest histogram entropy and brightness near ideal[theobjects.com](https://www.theobjects.com/dragonfly/dfhelp/2022-2/Content/Processing%20Images/Image%20Quality%20Metrics.htm#:~:text=Shannon%20Entropy). | Avoids poorly lit frames (too dark/bright). Often, the well-exposed frame coincides with proper focus (camera auto-exposure might have settled). | Lighting alone doesn't guarantee sharpness. Could pick a well-lit but blurry frame. Should be used in combination with other metrics rather than alone. |
| **Highest Detection Confidence** | Select the frame where the object detector's confidence is highest for the target object. Assumes running detection on each frame in the burst. | Leverages the AI model's judgement – a blurry or partial object will usually yield lower confidence, whereas a clear view yields high confidence. Ensures the object is clearly visible. | Requires running detection on all candidate frames (slower). Could be skewed if the model is uncertain due to occlusion or only sees part of the object (e.g., a hand partially visible might get lower confidence even if frame is sharp). Should verify the content of the frame, not just confidence. |
| **Composite Score (Multi-factor)** | Weighted combination of sharpness, motion, exposure, and detection info. For example, require a minimum sharpness and then among those pick highest confidence. | Holistic approach – balances all aspects of image quality and relevance. Tends to produce the most robust choice (clear and well-framed). | More complex to implement and tune. Needs careful weight selection or logic (could use rules like "discard any frame with blur score below X, then choose among remaining by confidence"). Slightly higher computation (calculating multiple metrics). |

Our recommendation is to use a **composite strategy**: first filter out frames that are definitely bad (e.g., blur score below a low threshold or severe motion), then among the remaining in the buffer, choose the one that maximizes a combination of sharpness and detection confidence (since those directly reflect clarity of the object of interest). In practice, the system could do something like: "from the last 5 frames where a hand was detected, save the one with the highest Laplacian variance *provided* its detection confidence is above a certain level (to ensure the hand is actually in frame)."" This ensures a baseline quality and the presence of the object.

All of this selection happens quickly – computing these metrics is lightweight. The buffer size is small, so the impact on memory is negligible (5 HD frames in memory). The result is that for each event (e.g., a person appears and moves their hand), you get **one well-chosen photo** rather than several mediocre ones or a randomly timed one. This is especially useful in a kitchen scenario: if a hand is grabbing an object, our system might detect the person and then pick the exact frame where the hand is clearly visible holding the object (sharp and centered) to save, instead of a blurry mid-motion frame.

The table above provides a rationale for the chosen approach by comparing alternatives. It's clear that combining criteria yields the best outcome in terms of image quality and relevance. We avoid single-criterion failures by cross-checking: the frame must be sharp *and* contain a confidently detected object, etc. This robust frame selection mechanism will greatly improve the quality of the captured dataset images.

## 5. Computer Vision Engineering Practices on Raspberry Pi (NCNN)

Deploying a real-time CV system on a Raspberry Pi requires careful engineering to maximize performance. Here we list best practices and optimizations specifically for Raspberry Pi + NCNN (and generally applicable to embedded CV):

- **Leverage NCNN's Efficiency:** NCNN is chosen for its optimised performance on ARM devices. Ensure you compile/use NCNN with all optimisations:
    - Use the latest NCNN library which enables NEON (ARM SIMD) and multi-threading. NCNN is highly optimised for mobile/embedded and can significantly outperform PyTorch on CPU for the same model. (Ultralytics reports YOLO models in NCNN format running an order of magnitude faster than PyTorch on Pi.)
    - Configure NCNN thread count to 4 (for Pi 4's four cores) or appropriate for Pi's cores. NCNN's `Extractor` can be set to use multiple threads, which will parallelise the convolution layers across cores.
    - Use FP16 inference if available: NCNN by default can use FP16 for storage and computations on ARM, which speeds up inference with minimal accuracy loss. This is usually enabled by default in NCNN builds for Pi.
    - **Model Simplification:** Use the smallest model that meets your needs. YOLOv11n (nano) is already a tiny model. Avoid larger variants (YOLOv11s/m) on the Pi, as they will slow down inference dramatically. If possible, reduce input size (see below) or prune the model if you train a custom one (remove classes not needed, etc.).
    - Ensure that the NCNN model files (`.param` and `.bin`) are loaded once and reused for each inference. The detector class should initialize the NCNN network in memory and keep it loaded (which the design allows). Loading models repeatedly would waste time and memory.
- **Model Warm-Up:** Perform a dummy inference at startup to **warm up caches and memory**. The first inference of a model is often slower due to one-time overhead (memory allocation, JIT initialisation if any). Running a forward pass on a blank image during initialization mitigates this[github.com](https://github.com/ultralytics/ultralytics/issues/3599#:~:text=The%20relevant%20code%20for%20loading,pass%20with%20a%20dummy%20input). This way, when the first real frame comes in, the model is already "warm" and performs at consistent speed.
- **Efficient Frame Preprocessing:** Resize frames to the model's expected input size as efficiently as possible. If YOLOv11n is using 640×640 input, feeding the full 1920×1080 image to the model is unnecessary and slow. Instead:
    - Downscale the frame (using OpenCV or even better, use the Piecamera2 preview stream at a lower resolution) to the required size before detection. OpenCV's `cv2.resize` is quite fast in C and will be much quicker than running the model on a large image.
    - Alternatively, if using the Ultralytics API for NCNN, the model might handle resizing internally (Ultralytics YOLO can auto-resize to 640). But it's good to control it to be sure.
    - **Use of dual streams:** Piecamera2 can output a full-resolution still and a lower-resolution preview simultaneously. Consider configuring Piecamera2 with a video stream (say 640×360 at 30 FPS) for detection and a still stream (1080p) for capture. This way, detection is always on small images (fast), and when you need to save, you can grab a full-res image at that moment. This is a more advanced configuration but would achieve both speed and quality. It prevents needing to resize down and then possibly up for saving (we always have the original for saving).
    - Minimize copies: ensure that the image data is not needlessly copied between threads. For example, when the capture thread puts a frame into the queue, it can pass the NumPy array reference. The inference thread should be careful not to create giant Python lists or do per-pixel Python loops. Use vectorised operations or OpenCV functions for any preprocessing (which are optimised).

- **Avoiding Bottlenecks in Camera Pipeline:** Piecamera2 in still mode has some delay (as seen with the 2s sleep). If higher FPS is needed, consider using the video mode of the camera which is optimised for continuous capture. You could run the camera at a fixed 30 FPS in 1080p video mode for smoother capture, rather than using still capture for each frame (which might be slower due to reconfiguration). The still mode was used for best quality; the trade-off is speed. If you need both quality and speed, one approach is: configure the camera for video (to get 10+ FPS at high res) and increase the shutter speed as mentioned. The image quality (in terms of ISP processing) might be slightly different than still mode, but likely still good. This is more of a camera tuning issue – the code architecture is already set to handle either as long as `Camera.read()` supplies frames at the desired rate.
- **Use Verified Libraries:** Continue using Ultralytics' infrastructure for model conversion and loading if it works (as in YOLO("yolo11n_ncnn_model")). These libraries handle a lot of low-level details. The conversion pipeline (YOLOv8 -> ONNX -> NCNN) as set up in the scripts is important. Make sure to test the NCNN model's accuracy against the PyTorch version to confirm the conversion did not break anything. Sometimes, small differences can occur (e.g., non-max suppression differences). If needed, apply the same NMS logic in post-processing to ensure consistency.
- **Profiling and Monitoring:** Employ profiling to find any slow spots. For example, if at runtime you find that 10 FPS is not reached, profile where the time is going (perhaps the capture or the saving is slower than expected). Tools like cProfile or even simple timestamp logging per section can identify if, say, `cv2.imwrite` is taking too long (maybe due to large image size). In that case, you might choose to save images at slightly lower resolution or quality (JPEG compression level) to speed it up. Another example: if `Picamera2.capture_array()` is slow at full res, consider using the asynchronous capture methods that allow the camera to run continuously and give you frames via callback, which can be faster.
- **Maintainability:** Keep each module independent and document the interfaces. For instance, document that `Detector.detect(frame)` returns a standardized result (list of detections with class, confidence, bbox). This will help if you integrate a new model (you'd implement the same interface). Use clear threading conventions (e.g., naming threads, proper exception handling in threads – perhaps use `threading.excepthook` to catch errors in threads, so the program doesn't silently hang if a thread crashes). Also, consider unit-testing the non-hardware parts of the code where possible (e.g., image_quality functions can be tested with sample images).

By following these practices, the system will be **performant, stable, and maintainable** on the Raspberry Pi:

- We exploit NCNN to get the best inference speed (as evidence, NCNN format gave huge speedups on Pi in benchmarks).
- We make efficient use of the Pi's limited resources (CPU, memory) by resizing frames, using multiple threads, and avoiding unnecessary work.
- We ensure longevity and reliability by using appropriate hardware settings (cooling, no undue wear on SD card) and software stability checks.
- The modular structure means future improvements (like upgrading to a Raspberry Pi 5 or swapping in a new model or adding a new sensor) can be done with minimal changes to the codebase.

All these strategic choices culminate in a system that can run real-time object detection on Raspberry Pi at ~10 FPS, capturing high-quality images of kitchen objects and hands, with a clean code architecture that is ready for future extensions. The emphasis on both design and engineering optimisations will make the system robust for deployment.